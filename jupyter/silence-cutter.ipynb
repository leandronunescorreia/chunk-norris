{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb119aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.platform == 'win32':\n",
    "    file_path = 'C:\\\\videos\\\\TheGreat_2_2001_169_2398_ProRes422HQ_ENG20_ENG51_BPO20_BPO51_Primary_A17913780.mov'\n",
    "elif sys.platform == 'linux':\n",
    "    file_path = '/mnt/c/videos/TheGreat_2_2001_169_2398_ProRes422HQ_ENG20_ENG51_BPO20_BPO51_Primary_A17913780.mov'\n",
    "else:\n",
    "    file_path = '/Users/leandro.correia/Documents/videos/TheGreat_2_2001_169_2398_ProRes422HQ_ENG20_ENG51_BPO20_BPO51_Primary_A17913780.mov'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454c9a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymediainfo import MediaInfo\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Extract media information\n",
    "media_info = MediaInfo.parse(file_path)\n",
    "\n",
    "# Initialize variables to store audio track count and metadata\n",
    "audio_tracks = []\n",
    "audio_metadata = []\n",
    "\n",
    "# Iterate through tracks to find audio tracks\n",
    "for track in media_info.tracks:\n",
    "    if track.track_type == \"Audio\":\n",
    "        audio_tracks.append(track)\n",
    "        audio_metadata.append(track.to_data())\n",
    "\n",
    "# Output the number of audio tracks and their metadata\n",
    "print(f\"Number of audio tracks: {len(audio_tracks)}\")\n",
    "print(\"Audio track metadata:\")\n",
    "for idx, metadata in enumerate(audio_metadata, start=1):\n",
    "    print(f\"Track {idx}: {metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32483203",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in audio_metadata[0].items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7cc06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "class SilenceNumpyDetector:\n",
    "    def __init__(self, threshold: float = 0.01, silence_duration: float = 0.5):\n",
    "        self.threshold = threshold\n",
    "        self.silence_duration = silence_duration  # in seconds\n",
    "\n",
    "    def detect_silence(self, buffer: np.ndarray, sample_rate: int) -> bool:\n",
    "        amplitude = np.mean(np.abs(buffer))\n",
    "        return amplitude < self.threshold\n",
    "\n",
    "    def find_nearest_silence(self, buffer: np.ndarray, sample_rate: int, search_start: int, search_end: int) -> int:\n",
    "        \"\"\"Search for 500ms silence between search_start and search_end (both in samples).\"\"\"\n",
    "        silence_samples = int(self.silence_duration * sample_rate)\n",
    "        step = int(sample_rate * 0.1)  # Slide every 100ms\n",
    "\n",
    "        best_candidate = None\n",
    "\n",
    "        for i in range(search_start, search_end - silence_samples, step):\n",
    "            window = buffer[i:i + silence_samples]\n",
    "            if self.detect_silence(window, sample_rate):\n",
    "                best_candidate = i\n",
    "                break  # Optionally: break to get first found, or keep searching to find nearest to 30s\n",
    "\n",
    "        return best_candidate\n",
    "\n",
    "    def split_buffer(self, input_data: np.ndarray, sample_rate: int) -> List[np.ndarray]:\n",
    "        max_duration = 30  # seconds\n",
    "        max_size_bytes = 25 * 1024 * 1024  # 25MB\n",
    "        silence_search_window = 10  # seconds before cut\n",
    "\n",
    "        chunks = []\n",
    "        cursor = 0\n",
    "        total_samples = len(input_data)\n",
    "\n",
    "        while cursor < total_samples:\n",
    "            target_samples = int(max_duration * sample_rate)\n",
    "            search_offset = int(silence_search_window * sample_rate)\n",
    "            target_end = cursor + target_samples\n",
    "\n",
    "            if target_end >= total_samples:\n",
    "                chunks.append(input_data[cursor:])\n",
    "                break\n",
    "\n",
    "            # Search for silence between (target_end - 10s) and (target_end)\n",
    "            search_start = max(cursor, target_end - search_offset)\n",
    "            search_end = target_end\n",
    "\n",
    "            silence_pos = self.find_nearest_silence(input_data, sample_rate, search_start, search_end)\n",
    "\n",
    "            if silence_pos is None:\n",
    "                cut_pos = target_end\n",
    "            else:\n",
    "                cut_pos = silence_pos\n",
    "\n",
    "            chunk = input_data[cursor:cut_pos]\n",
    "            estimated_size = chunk.nbytes\n",
    "\n",
    "            if estimated_size > max_size_bytes:\n",
    "                # fallback to size-based cut\n",
    "                max_samples = int(max_size_bytes / input_data.itemsize)\n",
    "                cut_pos = cursor + min(max_samples, target_samples)\n",
    "                chunk = input_data[cursor:cut_pos]\n",
    "\n",
    "            chunks.append(chunk)\n",
    "            cursor = cut_pos\n",
    "\n",
    "        return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fd618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "\n",
    "from extractor.audio_ffmpeg import AudioFFmpegExtractor\n",
    "from pymediainfo import MediaInfo\n",
    "\n",
    "\n",
    "SAMPLING_RATE = 16000\n",
    "\n",
    "# Extract media information\n",
    "media_info = MediaInfo.parse(file_path)\n",
    "\n",
    "# Initialize variables to store audio track count and metadata\n",
    "audio_tracks = []\n",
    "\n",
    "ffmpeg_extractor = AudioFFmpegExtractor()\n",
    "# Iterate through tracks to find audio tracks\n",
    "for track in media_info.tracks:\n",
    "    if track.track_type == \"Audio\":\n",
    "        buffer = ffmpeg_extractor.extract(file_path, track.to_data().get('stream_identifier'), SAMPLING_RATE)\n",
    "        \n",
    "        norm_flat_buffer = buffer.flatten().astype(np.float32) / 32768.0\n",
    "        audio_tracks.append({\n",
    "            'id': track.to_data().get('stream_identifier'),\n",
    "            'data': norm_flat_buffer,\n",
    "            'channel_layout': track.to_data().get('channel_layout')\n",
    "            })\n",
    "\n",
    "\n",
    "for audio in audio_tracks:\n",
    "    print(audio.get('id'), audio.get('channel_layout'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc0a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SilenceNumpyDetector(threshold=0.01)\n",
    "\n",
    "chunks = detector.split_buffer(audio_tracks[0].get('data'), sample_rate=16000)\n",
    "\n",
    "for i, chunk in enumerate(chunks):\n",
    "    duration_seconds = len(chunk) / 16000  # sample_rate\n",
    "    print(f\"Chunk {i}: {duration_seconds:.2f} seconds, {len(chunk)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "def plot_waveform(chunk, sample_rate, silence_sample_index=None, title_prefix=\"\"):\n",
    "    plt.figure(figsize=(7, 4))\n",
    "\n",
    "    # Waveform\n",
    "    time = np.linspace(0, len(chunk) / sample_rate, num=len(chunk))\n",
    "    plt.plot(time, chunk, alpha=0.7)\n",
    "    plt.title(f'{title_prefix} - Waveform')\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    if silence_sample_index is not None:\n",
    "        plt.axvline(silence_sample_index / sample_rate, color='red', linestyle='--', label=\"Silence Cut\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aa4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilenceNumpyDetector():\n",
    "    def __init__(self, threshold: float = 0.01):\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def split_buffer(self, audio: np.ndarray, sample_rate: int = 16000):\n",
    "        chunk_duration = 30  # seconds\n",
    "        min_silence_duration = 0.5  # seconds\n",
    "        pre_check_duration = 10  # seconds\n",
    "        max_bytes = 25 * 1024 * 1024  # 25 MB\n",
    "\n",
    "        max_samples = int((max_bytes / 2))  # int16 = 2 bytes\n",
    "        chunk_samples = int(chunk_duration * sample_rate)\n",
    "        pre_check_samples = int(pre_check_duration * sample_rate)\n",
    "        silence_samples = int(min_silence_duration * sample_rate)\n",
    "\n",
    "        chunks = []\n",
    "        silence_points = []  # store split points (in samples)\n",
    "\n",
    "        position = 0\n",
    "        while position < len(audio):\n",
    "            end_pos = min(position + chunk_samples, len(audio))\n",
    "\n",
    "            buffer = audio[position:end_pos]\n",
    "\n",
    "            # search for silence after pre-check\n",
    "            search_start = min(pre_check_samples, len(buffer) - silence_samples)\n",
    "            search_buffer = buffer[search_start:]\n",
    "            found = False\n",
    "            for i in range(0, len(search_buffer) - silence_samples):\n",
    "                segment = search_buffer[i:i + silence_samples]\n",
    "                if np.mean(np.abs(segment)) < self.threshold:\n",
    "                    split_sample = search_start + i\n",
    "                    chunks.append(buffer[:split_sample])\n",
    "                    silence_points.append(position + split_sample)\n",
    "                    position += split_sample\n",
    "                    found = True\n",
    "                    break\n",
    "            if not found:\n",
    "                chunks.append(buffer)\n",
    "                silence_points.append(position + len(buffer))\n",
    "                position += len(buffer)\n",
    "\n",
    "            if position * 2 > max_bytes:\n",
    "                break\n",
    "\n",
    "        return chunks, silence_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff105b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SilenceNumpyDetector(threshold=0.01)\n",
    "chunks, silence_points = detector.split_buffer(audio_tracks[0].get('data'), sample_rate=16000)\n",
    "\n",
    "start_sample = 0\n",
    "for i, (chunk, silence_sample) in enumerate(zip(chunks, silence_points)):\n",
    "    silence_relative = silence_sample - start_sample\n",
    "    plot_waveform(chunk, sample_rate=16000, silence_sample_index=silence_relative, title_prefix=f\"Chunk {i}\")\n",
    "    start_sample = silence_sample\n",
    "\n",
    "print(\"Split points (s):\", silence_points)\n",
    "print(\"Number of chunks:\", len(chunks))\n",
    "print(\"Chunk durations (s):\", [len(c) / 16000 for c in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103e13a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 4))\n",
    "full_audio = audio_tracks[0]['data']\n",
    "time = np.linspace(0, len(full_audio) / SAMPLING_RATE, num=len(full_audio))\n",
    "plt.plot(time, full_audio, alpha=0.7, label='Waveform')\n",
    "\n",
    "for sp in silence_points:\n",
    "    plt.axvline(sp / SAMPLING_RATE, color='red', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.title(\"Full Audio Waveform with Split Points\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.legend(['Waveform', 'Silence Split Points'])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41711b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "\n",
    "# Play each chunk in the notebook\n",
    "for i, chunk in enumerate(chunks):\n",
    "    print(f\"Playing chunk {i} ({len(chunk)/SAMPLING_RATE:.2f} seconds)\")\n",
    "    ipd.display(ipd.Audio(chunk, rate=SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91993f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SilencePointDetector:\n",
    "    def __init__(self, threshold=0.01, silence_duration=0.5):\n",
    "        self.threshold = threshold  # amplitude threshold\n",
    "        self.silence_duration = silence_duration  # min silence length (seconds)\n",
    "\n",
    "    def detect_silence_points(self, audio: np.ndarray, sample_rate: int = 16000):\n",
    "        silence_samples = int(self.silence_duration * sample_rate)\n",
    "        abs_audio = np.abs(audio)\n",
    "\n",
    "        # Detect contiguous silence spans using convolution\n",
    "        silence_mask = abs_audio < self.threshold\n",
    "        silence_convolved = np.convolve(silence_mask.astype(np.int32), \n",
    "                                        np.ones(silence_samples, dtype=np.int32), mode='valid')\n",
    "\n",
    "        # Indices where a silent span starts\n",
    "        silence_start_indices = np.where(silence_convolved == silence_samples)[0]\n",
    "\n",
    "        # Convert to seconds and avoid near-duplicates\n",
    "        silence_points = []\n",
    "        last_point = -float('inf')\n",
    "        min_gap = silence_samples  # avoid nearby silences\n",
    "\n",
    "        for idx in silence_start_indices:\n",
    "            if idx - last_point >= min_gap:\n",
    "                silence_points.append(idx / sample_rate)\n",
    "                last_point = idx\n",
    "\n",
    "        return silence_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bf86f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_silence_points(audio: np.ndarray, silence_points: list, sample_rate: int = 16000):\n",
    "    duration = len(audio) / sample_rate\n",
    "    times = np.linspace(0, duration, len(audio))\n",
    "\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    plt.plot(times, audio, label=\"Waveform\", alpha=0.7)\n",
    "    \n",
    "    for i, pos in enumerate(silence_points):\n",
    "        plt.axvline(x=pos, color='red', linestyle='--', label=\"Silence Split Points\" if i == 0 else \"\")\n",
    "    \n",
    "    plt.title(\"Full Audio Waveform with Split Points\")\n",
    "    plt.xlabel(\"Time (s)\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23927d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "detector = SilencePointDetector(threshold=0.01, silence_duration=0.5)\n",
    "silence_points = detector.detect_silence_points(audio_tracks[0]['data'], sample_rate=16000)\n",
    "\n",
    "plot_silence_points(audio_tracks[0]['data'], silence_points, sample_rate=16000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8fdd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SilencePointDetector:\n",
    "    def __init__(self, threshold=0.01, silence_duration=0.5, min_chunk_duration=10.0, max_chunk_duration=30.0):\n",
    "        self.threshold = threshold  # amplitude threshold for silence\n",
    "        self.silence_duration = silence_duration  # duration of silence to detect (in seconds)\n",
    "        self.min_chunk_duration = min_chunk_duration\n",
    "        self.max_chunk_duration = max_chunk_duration\n",
    "\n",
    "    def detect_silence_points(self, audio: np.ndarray, sample_rate: int = 16000):\n",
    "        silence_samples = int(self.silence_duration * sample_rate)\n",
    "        abs_audio = np.abs(audio)\n",
    "\n",
    "        # Create a mask of where the audio is silent\n",
    "        silence_mask = abs_audio < self.threshold\n",
    "\n",
    "        # Smooth the mask to find longer silent regions\n",
    "        silence_convolved = np.convolve(\n",
    "            silence_mask.astype(np.int32),\n",
    "            np.ones(silence_samples, dtype=np.int32),\n",
    "            mode='valid'\n",
    "        )\n",
    "\n",
    "        # Indices where silence starts\n",
    "        silence_start_indices = np.where(silence_convolved == silence_samples)[0]\n",
    "\n",
    "        # Convert sample indices to time (seconds)\n",
    "        all_silence_times = []\n",
    "        last_point = -float('inf')\n",
    "        for idx in silence_start_indices:\n",
    "            time = idx / sample_rate\n",
    "            if time - last_point >= self.silence_duration:\n",
    "                all_silence_times.append(time)\n",
    "                last_point = idx / sample_rate\n",
    "\n",
    "        # Now filter silence points to obey chunk duration limits\n",
    "        filtered_points = [0.0]  # always start at 0\n",
    "        last_time = 0.0\n",
    "\n",
    "        for t in all_silence_times:\n",
    "            gap = t - last_time\n",
    "            if self.min_chunk_duration <= gap <= self.max_chunk_duration:\n",
    "                filtered_points.append(t)\n",
    "                last_time = t\n",
    "            elif gap > self.max_chunk_duration:\n",
    "                # Insert a split at max_chunk_duration if silence is too far\n",
    "                forced_split = last_time + self.max_chunk_duration\n",
    "                filtered_points.append(forced_split)\n",
    "                last_time = forced_split\n",
    "\n",
    "        # Ensure final point\n",
    "        total_duration = len(audio) / sample_rate\n",
    "        if total_duration - last_time >= self.min_chunk_duration:\n",
    "            filtered_points.append(total_duration)\n",
    "\n",
    "        return filtered_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92be38ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SilencePointDetector(\n",
    "    threshold=0.01,\n",
    "    silence_duration=0.5,\n",
    "    min_chunk_duration=10.0,\n",
    "    max_chunk_duration=30.0\n",
    ")\n",
    "\n",
    "# Get split points (in seconds)\n",
    "split_points = detector.detect_silence_points(audio_tracks[0]['data'], 16000)\n",
    "\n",
    "print(\"Split points (s):\", split_points)\n",
    "plot_silence_points(audio_tracks[0]['data'], split_points, sample_rate=16000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chunk-norris",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
